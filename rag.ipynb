{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.youtube.com/watch?v=1DEtdr_oc9s\n",
      "[{'text': 'hi everyone so we are we are here to', 'start': 2.44, 'duration': 8.44}, {'text': 'present the pp for the logon so without', 'start': 5.68, 'duration': 7.72}, {'text': \"any further Ado let's start with the\", 'start': 10.88, 'duration': 5.239}, {'text': 'presentation so the problem statement a', 'start': 13.4, 'duration': 4.84}, {'text': 'problem statement is a self-learning a', 'start': 16.119, 'duration': 5.24}, {'text': 'power PDF to data converter uh our team', 'start': 18.24, 'duration': 5.44}, {'text': 'name is team conference the members of', 'start': 21.359, 'duration': 4.32}, {'text': 'our team are M', 'start': 23.68, 'duration': 6.439}, {'text': 'myself ban and PR we are from vishak', 'start': 25.679, 'duration': 6.88}, {'text': 'Institute of Technology', 'start': 30.119, 'duration': 6.041}, {'text': 'Pune so the idea approach and details so', 'start': 32.559, 'duration': 6.281}, {'text': 'a solution a proposed solution utilizes', 'start': 36.16, 'duration': 5.68}, {'text': 'a fine-tune multimodel that is lava', 'start': 38.84, 'duration': 5.44}, {'text': 'which is capable of vision capabilities', 'start': 41.84, 'duration': 6.28}, {'text': 'so it accepts uh image and text as a', 'start': 44.28, 'duration': 6.64}, {'text': 'input and machine learning to extract', 'start': 48.12, 'duration': 5.2}, {'text': 'data from complex documents so these', 'start': 50.92, 'duration': 5.0}, {'text': 'documents can be financial documents uh', 'start': 53.32, 'duration': 5.239}, {'text': 'Supply Chain management reports or', 'start': 55.92, 'duration': 5.319}, {'text': 'Healthcare reports any form of', 'start': 58.559, 'duration': 5.521}, {'text': 'structured PDF or any form of PDF', 'start': 61.239, 'duration': 5.961}, {'text': 'document so our solution leverages', 'start': 64.08, 'duration': 6.52}, {'text': 'generative a AI uh techniques to', 'start': 67.2, 'duration': 5.64}, {'text': 'understand layouts and content it', 'start': 70.6, 'duration': 4.519}, {'text': 'further cleans and pre-process the PDF', 'start': 72.84, 'duration': 5.04}, {'text': 'data then converts them into structured', 'start': 75.119, 'duration': 6.801}, {'text': 'format like Json and data frames so upon', 'start': 77.88, 'duration': 6.76}, {'text': 'converting into structured format will', 'start': 81.92, 'duration': 5.8}, {'text': 'be generating query based insights so', 'start': 84.64, 'duration': 6.439}, {'text': 'user will input some queries regarding', 'start': 87.72, 'duration': 7.56}, {'text': 'the extracted data and he will get the', 'start': 91.079, 'duration': 6.32}, {'text': 'specified answer accordingly from the', 'start': 95.28, 'duration': 5.68}, {'text': 'data itself and we are also uh', 'start': 97.399, 'duration': 6.481}, {'text': 'empowering up solution with promp based', 'start': 100.96, 'duration': 5.6}, {'text': 'visualization so user can asks the', 'start': 103.88, 'duration': 6.4}, {'text': 'questions or U insights about data in uh', 'start': 106.56, 'duration': 6.48}, {'text': 'and get output in form of visualizations', 'start': 110.28, 'duration': 5.199}, {'text': 'in form of line chart bar graph or pie', 'start': 113.04, 'duration': 5.6}, {'text': 'chart all other visualizations as well a', 'start': 115.479, 'duration': 5.521}, {'text': 'feedback look will allow user', 'start': 118.64, 'duration': 5.2}, {'text': 'corrections to improve system accuracy', 'start': 121.0, 'duration': 5.68}, {'text': 'over time so this is the component which', 'start': 123.84, 'duration': 5.919}, {'text': \"is used for self learning so we'll be\", 'start': 126.68, 'duration': 6.08}, {'text': 'fine-tuning the model uh on after a', 'start': 129.759, 'duration': 7.161}, {'text': 'particular uh span of time as we so the', 'start': 132.76, 'duration': 6.44}, {'text': 'key components of this solution includes', 'start': 136.92, 'duration': 5.28}, {'text': 'multimodel LM for intelligent data', 'start': 139.2, 'duration': 5.44}, {'text': 'extraction uh the next key component', 'start': 142.2, 'duration': 6.08}, {'text': 'that is uh that proves inefficient that', 'start': 144.64, 'duration': 6.84}, {'text': 'proves efficient for the accuracy to', 'start': 148.28, 'duration': 5.52}, {'text': 'upgrade over the time is feedback clue', 'start': 151.48, 'duration': 4.28}, {'text': 'for continuous learning so to', 'start': 153.8, 'duration': 5.0}, {'text': 'incorporate continuous learning uh we', 'start': 155.76, 'duration': 7.32}, {'text': 'have a pipeline which will uh fine tune', 'start': 158.8, 'duration': 8.0}, {'text': 'or retrain the lava llm to generate or', 'start': 163.08, 'duration': 6.879}, {'text': 'adapt with the new amount data that user', 'start': 166.8, 'duration': 7.76}, {'text': 'is uh working the solution upon next key', 'start': 169.959, 'duration': 7.161}, {'text': 'component is structured data output for', 'start': 174.56, 'duration': 4.28}, {'text': 'seamless in', 'start': 177.12, 'duration': 4.92}, {'text': 'integration moving further we are will', 'start': 178.84, 'duration': 6.319}, {'text': 'be storing this data and give potential', 'start': 182.04, 'duration': 6.96}, {'text': 'insights uh using the data', 'start': 185.159, 'duration': 7.16}, {'text': \"expected so let's move uh with the\", 'start': 189.0, 'duration': 7.239}, {'text': 'system flowcharts so in the system flow', 'start': 192.319, 'duration': 8.041}, {'text': 'charts uh using the UI of our solution', 'start': 196.239, 'duration': 6.401}, {'text': 'uh user will upload the PDF document', 'start': 200.36, 'duration': 5.84}, {'text': 'that he wants to extract data about and', 'start': 202.64, 'duration': 5.72}, {'text': 'then the document is pre-processed and', 'start': 206.2, 'duration': 6.88}, {'text': \"clean so converting into like a PDF it's\", 'start': 208.36, 'duration': 7.32}, {'text': 'a PDF converting into images doing some', 'start': 213.08, 'duration': 5.0}, {'text': 'pre-processing and this images will be', 'start': 215.68, 'duration': 5.72}, {'text': 'sent as an input to our fine tune', 'start': 218.08, 'duration': 6.799}, {'text': 'multimodel lava that is Vision uh', 'start': 221.4, 'duration': 6.88}, {'text': 'language model so this fine tune visual', 'start': 224.879, 'duration': 6.72}, {'text': 'language model is uh is trained on', 'start': 228.28, 'duration': 6.879}, {'text': 'images of the uh invoices', 'start': 231.599, 'duration': 8.72}, {'text': 'then HR reports or any of the data needs', 'start': 235.159, 'duration': 7.8}, {'text': 'to be extracted upon and with its', 'start': 240.319, 'duration': 5.48}, {'text': 'subsequent counterpart of the extracted', 'start': 242.959, 'duration': 4.961}, {'text': 'form it should give or structured form', 'start': 245.799, 'duration': 5.321}, {'text': 'of the data it should give the output in', 'start': 247.92, 'duration': 7.76}, {'text': 'so this images goes into the multi', 'start': 251.12, 'duration': 8.44}, {'text': 'multimodel uh fine tune multimodel so', 'start': 255.68, 'duration': 7.6}, {'text': 'this the image or the PDF may have data', 'start': 259.56, 'duration': 6.56}, {'text': 'from text tables and figures so we have', 'start': 263.28, 'duration': 6.24}, {'text': 'separate pipelines to extract data from', 'start': 266.12, 'duration': 4.32}, {'text': 'them', 'start': 269.52, 'duration': 3.44}, {'text': 'so rather be it insides from the figure', 'start': 270.44, 'duration': 5.08}, {'text': 'tables and the textual', 'start': 272.96, 'duration': 5.959}, {'text': 'form so this will be combine to generate', 'start': 275.52, 'duration': 6.28}, {'text': 'structured data in Json format using', 'start': 278.919, 'duration': 4.761}, {'text': 'pre-processing uh different', 'start': 281.8, 'duration': 4.8}, {'text': 'pre-processing techniques and data frame', 'start': 283.68, 'duration': 5.28}, {'text': 'so we are storing it into form of data', 'start': 286.6, 'duration': 5.84}, {'text': 'frame as well to give insights about uh', 'start': 288.96, 'duration': 6.44}, {'text': 'so about the data as well so next step', 'start': 292.44, 'duration': 6.96}, {'text': 'is generating insights so with insights', 'start': 295.4, 'duration': 6.04}, {'text': 'there are two options query base inside', 'start': 299.4, 'duration': 4.32}, {'text': 'from the data user can ask questions', 'start': 301.44, 'duration': 4.92}, {'text': 'about data like what is the data you are', 'start': 303.72, 'duration': 4.56}, {'text': 'extracted about give summary of the data', 'start': 306.36, 'duration': 5.52}, {'text': 'kind of uh things you normally ask a', 'start': 308.28, 'duration': 6.0}, {'text': 'chat bot about or chat interface like', 'start': 311.88, 'duration': 5.599}, {'text': 'chat chat GT like next is prompt based', 'start': 314.28, 'duration': 6.28}, {'text': 'visualization about the data to generate', 'start': 317.479, 'duration': 5.881}, {'text': 'Dynamic visualizations that represents', 'start': 320.56, 'duration': 6.56}, {'text': 'what is the data about and after this uh', 'start': 323.36, 'duration': 6.72}, {'text': 'user will be uh asked for a user', 'start': 327.12, 'duration': 6.4}, {'text': 'feedback so if the user gives a feedback', 'start': 330.08, 'duration': 5.92}, {'text': 'upon improving like the model performing', 'start': 333.52, 'duration': 5.36}, {'text': 'wrong in this this use case then uh will', 'start': 336.0, 'duration': 5.96}, {'text': \"be uh if it gives the feedback we'll use\", 'start': 338.88, 'duration': 6.159}, {'text': 'the feedback and the data to improve the', 'start': 341.96, 'duration': 6.519}, {'text': 'our uh machine learning models uh lava', 'start': 345.039, 'duration': 5.681}, {'text': 'model and if it does not give the', 'start': 348.479, 'duration': 4.761}, {'text': \"feedback then we'll uh move with the\", 'start': 350.72, 'duration': 5.479}, {'text': 'further uh process without giving any', 'start': 353.24, 'duration': 4.679}, {'text': 'flagging about', 'start': 356.199, 'duration': 4.681}, {'text': 'the wrong things that model might be', 'start': 357.919, 'duration': 6.321}, {'text': 'gone bias about and after going through', 'start': 360.88, 'duration': 5.8}, {'text': 'this feedback mechanism we are storing', 'start': 364.24, 'duration': 5.959}, {'text': 'the data uh used to uh for further', 'start': 366.68, 'duration': 5.72}, {'text': 'reference so this is the overall', 'start': 370.199, 'duration': 4.84}, {'text': 'flowchart or architectural diagram of a', 'start': 372.4, 'duration': 3.88}, {'text': 'propose', 'start': 375.039, 'duration': 4.201}, {'text': \"system so let's move with the use cases\", 'start': 376.28, 'duration': 5.88}, {'text': 'that it has to CER or serve', 'start': 379.24, 'duration': 6.12}, {'text': 'about so first is Finance and Accounting', 'start': 382.16, 'duration': 6.96}, {'text': 'so in finance it is invoice processing', 'start': 385.36, 'duration': 6.72}, {'text': 'automat IC Ally extract vendor info and', 'start': 389.12, 'duration': 5.68}, {'text': 'item details that might be in the', 'start': 392.08, 'duration': 5.72}, {'text': 'invoices uh to get prices for a stream', 'start': 394.8, 'duration': 5.76}, {'text': 'Lan uh accounts and payables second is', 'start': 397.8, 'duration': 4.6}, {'text': 'financial report analysis so convert', 'start': 400.56, 'duration': 4.12}, {'text': 'financial statements into structured', 'start': 402.4, 'duration': 4.72}, {'text': 'data for further analy and forecasting', 'start': 404.68, 'duration': 4.959}, {'text': 'upon the data so it can be a part of a', 'start': 407.12, 'duration': 4.88}, {'text': 'big pipeline as data', 'start': 409.639, 'duration': 5.321}, {'text': 'collection for these different use cases', 'start': 412.0, 'duration': 5.199}, {'text': 'next is Supply Chain management so', 'start': 414.96, 'duration': 4.6}, {'text': 'purchase order automations can be done', 'start': 417.199, 'duration': 5.72}, {'text': 'conver supplier orders uh into digital', 'start': 419.56, 'duration': 6.56}, {'text': 'format for inventory management shipping', 'start': 422.919, 'duration': 5.4}, {'text': 'the do shipping documentation so here', 'start': 426.12, 'duration': 5.919}, {'text': 'the use cases like extract relevant info', 'start': 428.319, 'duration': 5.88}, {'text': 'from bills uh in order to track', 'start': 432.039, 'duration': 5.0}, {'text': 'shipments effectively and efficiently', 'start': 434.199, 'duration': 5.521}, {'text': 'third use case that we provide uh is', 'start': 437.039, 'duration': 5.72}, {'text': 'legal and compilance so in that passes', 'start': 439.72, 'duration': 6.039}, {'text': 'contract to extract terms from', 'start': 442.759, 'duration': 6.72}, {'text': 'the uh privacy policy or uh cont', 'start': 445.759, 'duration': 6.081}, {'text': 'contract aing compilance and risk', 'start': 449.479, 'duration': 5.641}, {'text': 'management so it can be looked upon as', 'start': 451.84, 'duration': 7.12}, {'text': 'well converts regulatory dogs uh for', 'start': 455.12, 'duration': 6.96}, {'text': 'adherance to Import and Export rules we', 'start': 458.96, 'duration': 5.6}, {'text': 'can get information that also in the', 'start': 462.08, 'duration': 5.2}, {'text': 'structured format the next use case is', 'start': 464.56, 'duration': 6.12}, {'text': 'human resource so using in resume', 'start': 467.28, 'duration': 6.56}, {'text': 'passing uh extract content contact', 'start': 470.68, 'duration': 5.079}, {'text': 'details education work and his work', 'start': 473.84, 'duration': 4.52}, {'text': 'profile so for candidate sking can be', 'start': 475.759, 'duration': 5.44}, {'text': 'achieved using our system as well so', 'start': 478.36, 'duration': 5.6}, {'text': 'employee data management so convert HR', 'start': 481.199, 'duration': 5.12}, {'text': 'forms for easier storage and Analysis', 'start': 483.96, 'duration': 6.679}, {'text': \"can be done as well so let's go a look\", 'start': 486.319, 'duration': 6.201}, {'text': 'around what the Technologies we are', 'start': 490.639, 'duration': 4.481}, {'text': 'planning to use for the same so to', 'start': 492.52, 'duration': 5.519}, {'text': 'create the front end UI part we are uh', 'start': 495.12, 'duration': 5.4}, {'text': 'using stream lit so it will provide us a', 'start': 498.039, 'duration': 6.321}, {'text': 'easy interface and scalable as scalable', 'start': 500.52, 'duration': 5.44}, {'text': 'as well user friendly approach for the', 'start': 504.36, 'duration': 5.279}, {'text': \"user so we'll be using PDF minor so\", 'start': 505.96, 'duration': 6.12}, {'text': 'converting PDF into images and', 'start': 509.639, 'duration': 5.161}, {'text': 'processing upon them it is used for that', 'start': 512.08, 'duration': 5.68}, {'text': 'use case so to integrate generative AI', 'start': 514.8, 'duration': 4.919}, {'text': 'functionalities like summarizing giving', 'start': 517.76, 'duration': 4.44}, {'text': 'insights about the data so will be', 'start': 519.719, 'duration': 5.761}, {'text': 'incorporating latest State Technology', 'start': 522.2, 'duration': 6.12}, {'text': \"Lang chain as well we'll be fine tuning\", 'start': 525.48, 'duration': 5.24}, {'text': 'multimodel lava that is Vision language', 'start': 528.32, 'duration': 4.959}, {'text': 'model upon', 'start': 530.72, 'duration': 7.64}, {'text': 'uh the data which it can uh accept that', 'start': 533.279, 'duration': 8.041}, {'text': 'we want to generate structured data so', 'start': 538.36, 'duration': 6.88}, {'text': 'it will be fine tune on that data uh', 'start': 541.32, 'duration': 6.28}, {'text': 'next is Panda so it will be used to give', 'start': 545.24, 'duration': 5.159}, {'text': 'query based insight and also prom based', 'start': 547.6, 'duration': 5.4}, {'text': \"visualization so we'll use Panda for\", 'start': 550.399, 'duration': 5.201}, {'text': 'that for the technology for this whole', 'start': 553.0, 'duration': 5.2}, {'text': 'back end will be powered by Python and', 'start': 555.6, 'duration': 4.4}, {'text': 'the most important self learning or', 'start': 558.2, 'duration': 4.759}, {'text': 'human feedback learning so we we are in', 'start': 560.0, 'duration': 5.399}, {'text': 'incorporating fine-tuning this model', 'start': 562.959, 'duration': 4.681}, {'text': 'again on the data collected from the', 'start': 565.399, 'duration': 4.201}, {'text': 'human feedback and this will will be', 'start': 567.64, 'duration': 5.879}, {'text': 'done over a span of over a time so after', 'start': 569.6, 'duration': 8.359}, {'text': '20 to 30 days uh the the uh the fine', 'start': 573.519, 'duration': 6.841}, {'text': 'tuning will will do the fine tuning', 'start': 577.959, 'duration': 4.361}, {'text': 'again and the model will be familiar', 'start': 580.36, 'duration': 3.4}, {'text': 'with the current trends and the', 'start': 582.32, 'duration': 4.48}, {'text': 'documents it should parse or uh provide', 'start': 583.76, 'duration': 6.4}, {'text': 'structure data about so this was our', 'start': 586.8, 'duration': 5.8}, {'text': 'presentation a proposed solution for the', 'start': 590.16, 'duration': 6.32}, {'text': 'logon thank you', 'start': 592.6, 'duration': 3.88}]  hi everyone so we are we are here to present the pp for the logon so without any further Ado let's start with the presentation so the problem statement a problem statement is a self-learning a power PDF to data converter uh our team name is team conference the members of our team are M myself ban and PR we are from vishak Institute of Technology Pune so the idea approach and details so a solution a proposed solution utilizes a fine-tune multimodel that is lava which is capable of vision capabilities so it accepts uh image and text as a input and machine learning to extract data from complex documents so these documents can be financial documents uh Supply Chain management reports or Healthcare reports any form of structured PDF or any form of PDF document so our solution leverages generative a AI uh techniques to understand layouts and content it further cleans and pre-process the PDF data then converts them into structured format like Json and data frames so upon converting into structured format will be generating query based insights so user will input some queries regarding the extracted data and he will get the specified answer accordingly from the data itself and we are also uh empowering up solution with promp based visualization so user can asks the questions or U insights about data in uh and get output in form of visualizations in form of line chart bar graph or pie chart all other visualizations as well a feedback look will allow user corrections to improve system accuracy over time so this is the component which is used for self learning so we'll be fine-tuning the model uh on after a particular uh span of time as we so the key components of this solution includes multimodel LM for intelligent data extraction uh the next key component that is uh that proves inefficient that proves efficient for the accuracy to upgrade over the time is feedback clue for continuous learning so to incorporate continuous learning uh we have a pipeline which will uh fine tune or retrain the lava llm to generate or adapt with the new amount data that user is uh working the solution upon next key component is structured data output for seamless in integration moving further we are will be storing this data and give potential insights uh using the data expected so let's move uh with the system flowcharts so in the system flow charts uh using the UI of our solution uh user will upload the PDF document that he wants to extract data about and then the document is pre-processed and clean so converting into like a PDF it's a PDF converting into images doing some pre-processing and this images will be sent as an input to our fine tune multimodel lava that is Vision uh language model so this fine tune visual language model is uh is trained on images of the uh invoices then HR reports or any of the data needs to be extracted upon and with its subsequent counterpart of the extracted form it should give or structured form of the data it should give the output in so this images goes into the multi multimodel uh fine tune multimodel so this the image or the PDF may have data from text tables and figures so we have separate pipelines to extract data from them so rather be it insides from the figure tables and the textual form so this will be combine to generate structured data in Json format using pre-processing uh different pre-processing techniques and data frame so we are storing it into form of data frame as well to give insights about uh so about the data as well so next step is generating insights so with insights there are two options query base inside from the data user can ask questions about data like what is the data you are extracted about give summary of the data kind of uh things you normally ask a chat bot about or chat interface like chat chat GT like next is prompt based visualization about the data to generate Dynamic visualizations that represents what is the data about and after this uh user will be uh asked for a user feedback so if the user gives a feedback upon improving like the model performing wrong in this this use case then uh will be uh if it gives the feedback we'll use the feedback and the data to improve the our uh machine learning models uh lava model and if it does not give the feedback then we'll uh move with the further uh process without giving any flagging about the wrong things that model might be gone bias about and after going through this feedback mechanism we are storing the data uh used to uh for further reference so this is the overall flowchart or architectural diagram of a propose system so let's move with the use cases that it has to CER or serve about so first is Finance and Accounting so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream Lan uh accounts and payables second is financial report analysis so convert financial statements into structured data for further analy and forecasting upon the data so it can be a part of a big pipeline as data collection for these different use cases next is Supply Chain management so purchase order automations can be done conver supplier orders uh into digital format for inventory management shipping the do shipping documentation so here the use cases like extract relevant info from bills uh in order to track shipments effectively and efficiently third use case that we provide uh is legal and compilance so in that passes contract to extract terms from the uh privacy policy or uh cont contract aing compilance and risk management so it can be looked upon as well converts regulatory dogs uh for adherance to Import and Export rules we can get information that also in the structured format the next use case is human resource so using in resume passing uh extract content contact details education work and his work profile so for candidate sking can be achieved using our system as well so employee data management so convert HR forms for easier storage and Analysis can be done as well so let's go a look around what the Technologies we are planning to use for the same so to create the front end UI part we are uh using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so converting PDF into images and processing upon them it is used for that use case so to integrate generative AI functionalities like summarizing giving insights about the data so will be incorporating latest State Technology Lang chain as well we'll be fine tuning multimodel lava that is Vision language model upon uh the data which it can uh accept that we want to generate structured data so it will be fine tune on that data uh next is Panda so it will be used to give query based insight and also prom based visualization so we'll use Panda for that for the technology for this whole back end will be powered by Python and the most important self learning or human feedback learning so we we are in incorporating fine-tuning this model again on the data collected from the human feedback and this will will be done over a span of over a time so after 20 to 30 days uh the the uh the fine tuning will will do the fine tuning again and the model will be familiar with the current trends and the documents it should parse or uh provide structure data about so this was our presentation a proposed solution for the logon thank you\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def get_video_transcript(url):\n",
    "    video_id = url.replace('https://www.youtube.com/watch?v=', '')\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    clean_transcript = ''\n",
    "    for x in transcript:\n",
    "        sentence = x['text']\n",
    "        clean_transcript += f' {sentence}'\n",
    "    return transcript, clean_transcript\n",
    "\n",
    "url = 'https://www.youtube.com/watch?v=1DEtdr_oc9s'\n",
    "\n",
    "text, clean_transcript = get_video_transcript(url)\n",
    "\n",
    "\n",
    "print(url)\n",
    "print(text, clean_transcript)\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "51\n",
      "[{'text': \"hi everyone so we are we are here to present the pp for the logon so without any further Ado let's start with the presentation so the problem statement a\", 'start': '00:2.44', 'end': '00:16.12'}, {'text': 'problem statement is a self-learning a power PDF to data converter uh our team name is team conference the members of our team are M', 'start': '00:16.12', 'end': '00:25.68'}, {'text': 'myself ban and PR we are from vishak Institute of Technology Pune so the idea approach and details so a solution a proposed solution utilizes', 'start': '00:25.68', 'end': '00:38.84'}, {'text': 'a fine-tune multimodel that is lava which is capable of vision capabilities so it accepts uh image and text as a input and machine learning to extract', 'start': '00:38.84', 'end': '00:50.92'}, {'text': 'data from complex documents so these documents can be financial documents uh Supply Chain management reports or Healthcare reports any form of', 'start': '00:50.92', 'end': '01:1.24'}, {'text': 'structured PDF or any form of PDF document so our solution leverages generative a AI uh techniques to understand layouts and content it', 'start': '01:1.24', 'end': '01:12.84'}, {'text': 'further cleans and pre-process the PDF data then converts them into structured format like Json and data frames so upon converting into structured format will', 'start': '01:12.84', 'end': '01:24.64'}, {'text': 'be generating query based insights so user will input some queries regarding the extracted data and he will get the specified answer accordingly from the', 'start': '01:24.64', 'end': '01:37.40'}, {'text': 'data itself and we are also uh empowering up solution with promp based visualization so user can asks the questions or U insights about data in uh', 'start': '01:37.40', 'end': '01:50.28'}, {'text': 'and get output in form of visualizations in form of line chart bar graph or pie chart all other visualizations as well a feedback look will allow user', 'start': '01:50.28', 'end': '02:1.00'}, {'text': \"corrections to improve system accuracy over time so this is the component which is used for self learning so we'll be fine-tuning the model uh on after a\", 'start': '02:1.00', 'end': '02:12.76'}, {'text': 'particular uh span of time as we so the key components of this solution includes multimodel LM for intelligent data extraction uh the next key component', 'start': '02:12.76', 'end': '02:24.64'}, {'text': 'that is uh that proves inefficient that proves efficient for the accuracy to upgrade over the time is feedback clue for continuous learning so to', 'start': '02:24.64', 'end': '02:35.76'}, {'text': 'incorporate continuous learning uh we have a pipeline which will uh fine tune or retrain the lava llm to generate or adapt with the new amount data that user', 'start': '02:35.76', 'end': '02:49.96'}, {'text': 'is uh working the solution upon next key component is structured data output for seamless in integration moving further we are will', 'start': '02:49.96', 'end': '03:2.04'}, {'text': \"be storing this data and give potential insights uh using the data expected so let's move uh with the system flowcharts so in the system flow\", 'start': '03:2.04', 'end': '03:16.24'}, {'text': 'charts uh using the UI of our solution uh user will upload the PDF document that he wants to extract data about and then the document is pre-processed and', 'start': '03:16.24', 'end': '03:28.36'}, {'text': \"clean so converting into like a PDF it's a PDF converting into images doing some pre-processing and this images will be sent as an input to our fine tune\", 'start': '03:28.36', 'end': '03:41.40'}, {'text': 'multimodel lava that is Vision uh language model so this fine tune visual language model is uh is trained on images of the uh invoices', 'start': '03:41.40', 'end': '03:55.16'}, {'text': 'then HR reports or any of the data needs to be extracted upon and with its subsequent counterpart of the extracted form it should give or structured form', 'start': '03:55.16', 'end': '04:7.92'}, {'text': 'of the data it should give the output in so this images goes into the multi multimodel uh fine tune multimodel so this the image or the PDF may have data', 'start': '04:7.92', 'end': '04:23.28'}, {'text': 'from text tables and figures so we have separate pipelines to extract data from them so rather be it insides from the figure', 'start': '04:23.28', 'end': '04:32.96'}, {'text': 'tables and the textual form so this will be combine to generate structured data in Json format using pre-processing uh different', 'start': '04:32.96', 'end': '04:43.68'}, {'text': 'pre-processing techniques and data frame so we are storing it into form of data frame as well to give insights about uh so about the data as well so next step', 'start': '04:43.68', 'end': '04:55.40'}, {'text': 'is generating insights so with insights there are two options query base inside from the data user can ask questions about data like what is the data you are', 'start': '04:55.40', 'end': '05:6.36'}, {'text': 'extracted about give summary of the data kind of uh things you normally ask a chat bot about or chat interface like chat chat GT like next is prompt based', 'start': '05:6.36', 'end': '05:17.48'}, {'text': 'visualization about the data to generate Dynamic visualizations that represents what is the data about and after this uh user will be uh asked for a user', 'start': '05:17.48', 'end': '05:30.08'}, {'text': \"feedback so if the user gives a feedback upon improving like the model performing wrong in this this use case then uh will be uh if it gives the feedback we'll use\", 'start': '05:30.08', 'end': '05:41.96'}, {'text': \"the feedback and the data to improve the our uh machine learning models uh lava model and if it does not give the feedback then we'll uh move with the\", 'start': '05:41.96', 'end': '05:53.24'}, {'text': 'further uh process without giving any flagging about the wrong things that model might be gone bias about and after going through', 'start': '05:53.24', 'end': '06:4.24'}, {'text': 'this feedback mechanism we are storing the data uh used to uh for further reference so this is the overall flowchart or architectural diagram of a', 'start': '06:4.24', 'end': '06:15.04'}, {'text': \"propose system so let's move with the use cases that it has to CER or serve about so first is Finance and Accounting\", 'start': '06:15.04', 'end': '06:25.36'}, {'text': 'so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream', 'start': '06:25.36', 'end': '06:37.80'}, {'text': 'Lan uh accounts and payables second is financial report analysis so convert financial statements into structured data for further analy and forecasting', 'start': '06:37.80', 'end': '06:47.12'}, {'text': 'upon the data so it can be a part of a big pipeline as data collection for these different use cases next is Supply Chain management so', 'start': '06:47.12', 'end': '06:57.20'}, {'text': 'purchase order automations can be done conver supplier orders uh into digital format for inventory management shipping the do shipping documentation so here', 'start': '06:57.20', 'end': '07:8.32'}, {'text': 'the use cases like extract relevant info from bills uh in order to track shipments effectively and efficiently third use case that we provide uh is', 'start': '07:8.32', 'end': '07:19.72'}, {'text': 'legal and compilance so in that passes contract to extract terms from the uh privacy policy or uh cont contract aing compilance and risk', 'start': '07:19.72', 'end': '07:31.84'}, {'text': 'management so it can be looked upon as well converts regulatory dogs uh for adherance to Import and Export rules we can get information that also in the', 'start': '07:31.84', 'end': '07:44.56'}, {'text': 'structured format the next use case is human resource so using in resume passing uh extract content contact details education work and his work', 'start': '07:44.56', 'end': '07:55.76'}, {'text': 'profile so for candidate sking can be achieved using our system as well so employee data management so convert HR forms for easier storage and Analysis', 'start': '07:55.76', 'end': '08:6.32'}, {'text': \"can be done as well so let's go a look around what the Technologies we are planning to use for the same so to create the front end UI part we are uh\", 'start': '08:6.32', 'end': '08:18.04'}, {'text': \"using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so\", 'start': '08:18.04', 'end': '08:29.64'}, {'text': 'converting PDF into images and processing upon them it is used for that use case so to integrate generative AI functionalities like summarizing giving', 'start': '08:29.64', 'end': '08:39.72'}, {'text': \"insights about the data so will be incorporating latest State Technology Lang chain as well we'll be fine tuning multimodel lava that is Vision language\", 'start': '08:39.72', 'end': '08:50.72'}, {'text': 'model upon uh the data which it can uh accept that we want to generate structured data so it will be fine tune on that data uh', 'start': '08:50.72', 'end': '09:5.24'}, {'text': \"next is Panda so it will be used to give query based insight and also prom based visualization so we'll use Panda for that for the technology for this whole\", 'start': '09:5.24', 'end': '09:15.60'}, {'text': 'back end will be powered by Python and the most important self learning or human feedback learning so we we are in incorporating fine-tuning this model', 'start': '09:15.60', 'end': '09:25.40'}, {'text': 'again on the data collected from the human feedback and this will will be done over a span of over a time so after 20 to 30 days uh the the uh the fine', 'start': '09:25.40', 'end': '09:37.96'}, {'text': 'tuning will will do the fine tuning again and the model will be familiar with the current trends and the documents it should parse or uh provide', 'start': '09:37.96', 'end': '09:46.80'}, {'text': 'structure data about so this was our presentation a proposed solution for the logon thank you', 'start': '09:46.80', 'end': '09:46.80'}]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def format_time(seconds):\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    return f\"{int(minutes):02d}:{seconds:.2f}\"\n",
    "\n",
    "def time_to_seconds(time_str):\n",
    "    minutes, seconds = map(float, time_str.split(':'))\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "combined_segments = []\n",
    "group_size = 4 \n",
    "\n",
    "for i in range(0, len(text), group_size):\n",
    "    group_text_segments = [item['text'] for item in text[i:i+group_size]]\n",
    "    combined_text_segment = ' '.join(group_text_segments)\n",
    "\n",
    "    start_times = [item['start'] for item in text[i:i+group_size]]\n",
    "    min_start_time = min(start_times)\n",
    "\n",
    "    combined_segments.append({\n",
    "        'text': combined_text_segment,\n",
    "        'start': format_time(round(min_start_time, 2)),\n",
    "        'end': format_time(round(min_start_time, 2))\n",
    "    })\n",
    "\n",
    "for i in range(len(combined_segments) - 1):\n",
    "    next_start_time = time_to_seconds(combined_segments[i + 1]['start'])\n",
    "    combined_segments[i]['end'] = format_time(round(next_start_time, 2))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(type(combined_segments))\n",
    "print(len(combined_segments))\n",
    "print(combined_segments)\n",
    "# for segment in combined_segments:\n",
    "#     print(segment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "djif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks generated and written to rag.txt!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\auth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def generate_chunks(response, time_format=\"mm:ss:hh\"):\n",
    "  \n",
    "  text_chunks = []\n",
    "  for element in response:\n",
    "    text = element['text']\n",
    "    start_time = element['start']\n",
    "    end_time = element['end']\n",
    "\n",
    "\n",
    "    # Split text into sentences (optional)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "      text_chunk = f\"{start_time} <= : {sentence} : => {end_time}\"\n",
    "      text_chunks.append(text_chunk)\n",
    "\n",
    "  return text_chunks\n",
    "\n",
    "# Assuming your API response is stored in a variable called 'response'\n",
    "chunks = generate_chunks(combined_segments)\n",
    "\n",
    "# Write the chunks to a file\n",
    "with open(\"rag.txt\", \"w\") as file:\n",
    "  file.write(\"\\n\".join(chunks))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Chunks generated and written to rag.txt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"00:2.44 <= : hi everyone so we are we are here to present the pp for the logon so without any further Ado let's start with the presentation so the problem statement a : => 00:16.12\\n00:16.12 <= : problem statement is a self-learning a power PDF to data converter uh our team name is team conference the members of our team are M : => 00:25.68\\n00:25.68 <= : myself ban and PR we are from vishak Institute of Technology Pune so the idea approach and details so a solution a proposed solution utilizes : => 00:38.84\\n00:38.84 <= : a fine-tune multimodel that is lava which is capable of vision capabilities so it accepts uh image and text as a input and machine learning to extract : => 00:50.92\\n00:50.92 <= : data from complex documents so these documents can be financial documents uh Supply Chain management reports or Healthcare reports any form of : => 01:1.24\\n01:1.24 <= : structured PDF or any form of PDF document so our solution leverages generative a AI uh techniques to understand layouts and content it : => 01:12.84\\n01:12.84 <= : further cleans and pre-process the PDF data then converts them into structured format like Json and data frames so upon converting into structured format will : => 01:24.64\\n01:24.64 <= : be generating query based insights so user will input some queries regarding the extracted data and he will get the specified answer accordingly from the : => 01:37.40\\n01:37.40 <= : data itself and we are also uh empowering up solution with promp based visualization so user can asks the questions or U insights about data in uh : => 01:50.28\\n01:50.28 <= : and get output in form of visualizations in form of line chart bar graph or pie chart all other visualizations as well a feedback look will allow user : => 02:1.00\\n02:1.00 <= : corrections to improve system accuracy over time so this is the component which is used for self learning so we'll be fine-tuning the model uh on after a : => 02:12.76\\n02:12.76 <= : particular uh span of time as we so the key components of this solution includes multimodel LM for intelligent data extraction uh the next key component : => 02:24.64\\n02:24.64 <= : that is uh that proves inefficient that proves efficient for the accuracy to upgrade over the time is feedback clue for continuous learning so to : => 02:35.76\\n02:35.76 <= : incorporate continuous learning uh we have a pipeline which will uh fine tune or retrain the lava llm to generate or adapt with the new amount data that user : => 02:49.96\\n02:49.96 <= : is uh working the solution upon next key component is structured data output for seamless in integration moving further we are will : => 03:2.04\\n03:2.04 <= : be storing this data and give potential insights uh using the data expected so let's move uh with the system flowcharts so in the system flow : => 03:16.24\\n03:16.24 <= : charts uh using the UI of our solution uh user will upload the PDF document that he wants to extract data about and then the document is pre-processed and : => 03:28.36\\n03:28.36 <= : clean so converting into like a PDF it's a PDF converting into images doing some pre-processing and this images will be sent as an input to our fine tune : => 03:41.40\\n03:41.40 <= : multimodel lava that is Vision uh language model so this fine tune visual language model is uh is trained on images of the uh invoices : => 03:55.16\\n03:55.16 <= : then HR reports or any of the data needs to be extracted upon and with its subsequent counterpart of the extracted form it should give or structured form : => 04:7.92\\n04:7.92 <= : of the data it should give the output in so this images goes into the multi multimodel uh fine tune multimodel so this the image or the PDF may have data : => 04:23.28\\n04:23.28 <= : from text tables and figures so we have separate pipelines to extract data from them so rather be it insides from the figure : => 04:32.96\\n04:32.96 <= : tables and the textual form so this will be combine to generate structured data in Json format using pre-processing uh different : => 04:43.68\\n04:43.68 <= : pre-processing techniques and data frame so we are storing it into form of data frame as well to give insights about uh so about the data as well so next step : => 04:55.40\\n04:55.40 <= : is generating insights so with insights there are two options query base inside from the data user can ask questions about data like what is the data you are : => 05:6.36\\n05:6.36 <= : extracted about give summary of the data kind of uh things you normally ask a chat bot about or chat interface like chat chat GT like next is prompt based : => 05:17.48\\n05:17.48 <= : visualization about the data to generate Dynamic visualizations that represents what is the data about and after this uh user will be uh asked for a user : => 05:30.08\\n05:30.08 <= : feedback so if the user gives a feedback upon improving like the model performing wrong in this this use case then uh will be uh if it gives the feedback we'll use : => 05:41.96\\n05:41.96 <= : the feedback and the data to improve the our uh machine learning models uh lava model and if it does not give the feedback then we'll uh move with the : => 05:53.24\\n05:53.24 <= : further uh process without giving any flagging about the wrong things that model might be gone bias about and after going through : => 06:4.24\\n06:4.24 <= : this feedback mechanism we are storing the data uh used to uh for further reference so this is the overall flowchart or architectural diagram of a : => 06:15.04\\n06:15.04 <= : propose system so let's move with the use cases that it has to CER or serve about so first is Finance and Accounting : => 06:25.36\\n06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80\\n06:37.80 <= : Lan uh accounts and payables second is financial report analysis so convert financial statements into structured data for further analy and forecasting : => 06:47.12\\n06:47.12 <= : upon the data so it can be a part of a big pipeline as data collection for these different use cases next is Supply Chain management so : => 06:57.20\\n06:57.20 <= : purchase order automations can be done conver supplier orders uh into digital format for inventory management shipping the do shipping documentation so here : => 07:8.32\\n07:8.32 <= : the use cases like extract relevant info from bills uh in order to track shipments effectively and efficiently third use case that we provide uh is : => 07:19.72\\n07:19.72 <= : legal and compilance so in that passes contract to extract terms from the uh privacy policy or uh cont contract aing compilance and risk : => 07:31.84\\n07:31.84 <= : management so it can be looked upon as well converts regulatory dogs uh for adherance to Import and Export rules we can get information that also in the : => 07:44.56\\n07:44.56 <= : structured format the next use case is human resource so using in resume passing uh extract content contact details education work and his work : => 07:55.76\\n07:55.76 <= : profile so for candidate sking can be achieved using our system as well so employee data management so convert HR forms for easier storage and Analysis : => 08:6.32\\n08:6.32 <= : can be done as well so let's go a look around what the Technologies we are planning to use for the same so to create the front end UI part we are uh : => 08:18.04\\n08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\\n08:29.64 <= : converting PDF into images and processing upon them it is used for that use case so to integrate generative AI functionalities like summarizing giving : => 08:39.72\\n08:39.72 <= : insights about the data so will be incorporating latest State Technology Lang chain as well we'll be fine tuning multimodel lava that is Vision language : => 08:50.72\\n08:50.72 <= : model upon uh the data which it can uh accept that we want to generate structured data so it will be fine tune on that data uh : => 09:5.24\\n09:5.24 <= : next is Panda so it will be used to give query based insight and also prom based visualization so we'll use Panda for that for the technology for this whole : => 09:15.60\\n09:15.60 <= : back end will be powered by Python and the most important self learning or human feedback learning so we we are in incorporating fine-tuning this model : => 09:25.40\\n09:25.40 <= : again on the data collected from the human feedback and this will will be done over a span of over a time so after 20 to 30 days uh the the uh the fine : => 09:37.96\\n09:37.96 <= : tuning will will do the fine tuning again and the model will be familiar with the current trends and the documents it should parse or uh provide : => 09:46.80\\n09:46.80 <= : structure data about so this was our presentation a proposed solution for the logon thank you : => 09:46.80\", metadata={'source': './rag.txt'})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"./rag.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1,\n",
    "    chunk_overlap  = 0,\n",
    "    length_function = lambda x: 1, # hack - usually len is used \n",
    "    is_separator_regex = False\n",
    ")\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "len(split_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"00:2.44 <= : hi everyone so we are we are here to present the pp for the logon so without any further Ado let's start with the presentation so the problem statement a : => 00:16.12\", metadata={'source': './rag.txt'}),\n",
       " Document(page_content='00:16.12 <= : problem statement is a self-learning a power PDF to data converter uh our team name is team conference the members of our team are M : => 00:25.68', metadata={'source': './rag.txt'}),\n",
       " Document(page_content='00:25.68 <= : myself ban and PR we are from vishak Institute of Technology Pune so the idea approach and details so a solution a proposed solution utilizes : => 00:38.84', metadata={'source': './rag.txt'}),\n",
       " Document(page_content='00:38.84 <= : a fine-tune multimodel that is lava which is capable of vision capabilities so it accepts uh image and text as a input and machine learning to extract : => 00:50.92', metadata={'source': './rag.txt'})]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split_docs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "db = Chroma.from_documents(split_docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(page_content=\"08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\", metadata={'source': './rag.txt'}), Document(page_content=\"08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\", metadata={'source': './rag.txt'}), Document(page_content='06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80', metadata={'source': './rag.txt'}), Document(page_content='06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80', metadata={'source': './rag.txt'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"Streamlit is used for ?\"\n",
    "rel_docs = db.similarity_search(query)\n",
    "\n",
    "\n",
    "# print results\n",
    "print(len(rel_docs))\n",
    "print(rel_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context. \\nThink step by step before providing a detailed answer. \\nI will tip you $1000 if the user finds the answer helpful. \\n<context>\\n{context}\\n</context>\\nQuestion: {input}'))])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\", metadata={'source': './rag.txt'})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rel_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\n",
      "08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\n",
      "06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80\n",
      "06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80\n",
      "\n",
      "What is Streamlit?\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\n",
    "for doc in rel_docs:\n",
    "    context += doc.page_content + \"\\n\"  \n",
    "\n",
    "question = \"What is Streamlit?\"\n",
    "\n",
    "\n",
    "\n",
    "print(context)\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the following question based only on the provided context. \n",
      "Think step by step before providing a detailed answer. \n",
      "I will tip you $1000 if the user finds the answer helpful. \n",
      "<context>\n",
      "08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\n",
      "08:18.04 <= : using stream lit so it will provide us a easy interface and scalable as scalable as well user friendly approach for the user so we'll be using PDF minor so : => 08:29.64\n",
      "06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80\n",
      "06:25.36 <= : so in finance it is invoice processing automat IC Ally extract vendor info and item details that might be in the invoices uh to get prices for a stream : => 06:37.80\n",
      "\n",
      "</context>\n",
      "Question: What is Streamlit?\n"
     ]
    }
   ],
   "source": [
    "prompt_formatted = prompt.format(\n",
    "    input=question,\n",
    "    context = context,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(prompt_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
